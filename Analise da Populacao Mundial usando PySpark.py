# -*- coding: utf-8 -*-
"""Projeto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JGNAgSePfZBdXRQ5myP4J5hlrQ0w1bEW
"""

#Instalações e imports

!pip install pyspark py4j
!pip install findspark

from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql.functions import * 
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import rand, randn
from pyspark.ml.regression import LinearRegression
from pyspark.ml.clustering import KMeans
from pyspark.ml.stat import Correlation
from google.colab import drive

import pyspark
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Criação da SparkSession
conf= SparkConf().setAppName('WorldPopulationPrediction').setMaster('local[*]')
sc= SparkContext.getOrCreate(conf=conf)
spark = SparkSession(sc)

#Mount da google drive
drive.mount('/content/drive')

#Configuração para o spark conseguir ler o ficheiro csv do dataset
df = spark.read.format("csv").option("header","true")\
.option("inferSchema","true").load("../content/drive/MyDrive/Colab Notebooks/pop_tot.csv")

#Leitura do ficheiro csv e atribuição à variável df
df=spark.read.csv('../content/drive/MyDrive/Colab Notebooks/pop_tot.csv',inferSchema=True,header=True)

#Verificação dos tipos de variáveis
df.dtypes

#Alteração do tipo de variável string -> int
year_string = df.columns[4:34]

for c in year_string:
  df = df.withColumn(c, F.col(c).cast('Integer'))

#Drop de 4 colunas e de quaisquer valores nulos
bd=df.drop('Country Name', 'Country Code', 'Indicator Name', 'Indicator Code').dropna(how ='any')

#Verificação de valores null
missing_values = []

for col in bd.columns:
    missing_values.append((col, bd.filter(bd[col].isNull()).count()))

for col, val in missing_values:
    if val == 0:
        print("{} : Nenhum missing values encontrado".format(col))
    else:
        print("{} : {} missing values".format(col, val))

#Criação do vetor
vector_col = 'features'

years=['1960','1961','1962','1963','1964','1965','1966','1967','1968','1969',
      '1970','1971','1972','1973','1974','1975','1976','1977','1978','1979',
      '1980','1981','1982','1983','1984','1985','1986','1987','1988','1989',
      '1990','1991','1992','1993','1994','1995','1996','1997','1998','1999',
      '2000','2001','2002','2003','2004','2005','2006','2007','2008','2009',
      '2010','2011','2012','2013','2014','2015','2016','2017','2018','2019',
      '2020','2021']

assembler = VectorAssembler(inputCols=years, outputCol=vector_col)

#Aplicação do vetor ao dataset em uso 
df_vector = assembler.transform(bd)

#Visualização do dataframe criado acima com a coluna do vetor
df_vector

#Visualização das primeiras 5 linhas da coluna features (vetor) 
df_vector.select('features').show(5,False)

#Cálculo da correlação entre variável
matrix = Correlation.corr(df_vector,vector_col).collect()[0][0]

#Criação da matrix
corrmatrix = matrix.toArray().tolist()

#Criação do dataframe para ver a correlação entre váriaveis (anos)
df_corr=spark.createDataFrame(corrmatrix, years)

#Visualização das primeiras 5 linhas das primeiras 5 colunas do dataframe de correlação
df_corr.select(df_corr.columns[0:5]).show(5)

#Criação do modelo para depois ser usado nos métodos de ML
model_df=df_vector.select('features','2021')

#Verificação de quantas linhas e colunas tem o nosso modelo
print((model_df.count(), len(model_df.columns)))

#RandomSplit dos dados de treino (70%) e de teste (30%)
train_df,test_df=model_df.randomSplit([0.7,0.3])

#Descrição do dataframe dos dados de treino
train_df.describe().show()

#Descrição do dataframe dos dados de teste
test_df.describe().show()

#Criação do objeto StandardScaler e aplicação nos conjuntos de treino e de teste
scaler = StandardScaler(inputCol='features', outputCol='standard_features')
scaler_model = scaler.fit(train_df)
train = scaler_model.transform(train_df)
test = scaler_model.transform(test_df)

#Visualização das primeiras 2 linhas da coluna standard_features de ambos os dataframes
train.select('standard_features').show(2,truncate=False)
test.select('standard_features').show(2,truncate=False)

"""## **Regressão Linear**"""

#Criação do objeto LinearRegression, treinamento do modelo com os dados de treino e realizações de previsões com os dados de teste
lin_Reg=LinearRegression(featuresCol='standard_features', labelCol='2021')
model = lin_Reg.fit(train)
predictions = model.transform(test)

#Realização das previsões passando para um dataframe (.toPandas())
y_actual = predictions.select('2021').toPandas()
y_pred = predictions.select('prediction').toPandas()

#Criação do gráfico dos resultados das predições do modelo
plt.figure(figsize=(15, 12))

plt.scatter(y_actual, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Linear Regression Results')
plt.show()

#Cálculo da inclinação dos dados, ou seja, se eles tiverem muita correlação vai ser perto de 0 e vice versa.
model.intercept

#Visualização dos coeficientes do modelo
model.coefficients

#Avaliação das previsões dos dados de treino 
training_predictions=model.evaluate(train)

#Média dos erros quadrados das previsões dos dados de treino
training_predictions.meanSquaredError

#Visualização de erros residuais baseados em previsões
test_results=model.evaluate(test)
test_results.residuals.show(10)

#Média dos erros quadrados dos resultados dos dados de teste
test_results.meanSquaredError

#Raiz quadrada da média dos erros quadrados dos resultados dos dados de teste
test_results.rootMeanSquaredError

#Cálculo do score do modelo ML 
Score=1-0.327768462516526
print(Score)

"""O nosso modelo de regressão linear tem uma precisão de 67.22%

## **Clustering**
"""

#Criação e aplicação do modelo KMeans aos dados de treino
errors=[]

for k in range(2,10):
    kmeans = KMeans(featuresCol='features',k=k)
    model = kmeans.fit(train)
    sse = model.summary.trainingCost
    errors.append(sse)
    print("With K={}".format(k))
    print("Within Set Sum of Squared Errors = " + str(sse))
    print('--'*30)

#Criação do gráfico de cotovelo
plt.figure(figsize=(12, 6))

plt.plot(range(2,10), errors)
plt.xlabel('Número de clusters')
plt.ylabel('Custo dos Errors')
plt.title('Gráfico de Cotovelo')
plt.show()

#Verificação dos dados dentro de cada clusters
centers = model.clusterCenters()
i=1
print("Cluster Centers: ")
for center in centers:
  print("Cluster " + str(i))
  print(center)
  i+=1